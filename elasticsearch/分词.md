
## 1.怎么看当前文本的分词结果

```powershell
# 指定analyzer
GET /yuan/_analyze
{
    "analyzer": "standard",
    "text": "e生保2020版保障什么内容"
}

# 通过告知typename明确analyzer
GET yuan/_analyze 
{
  "field": "_doc", 
  "text":  "e生保2020版和e生保2018版的区别是什么"
}
```

[https://www.elastic.co/guide/en/elasticsearch/reference/7.1/\_testing\_analyzers.html](https://www.elastic.co/guide/en/elasticsearch/reference/7.1/_testing_analyzers.html "https://www.elastic.co/guide/en/elasticsearch/reference/7.1/_testing_analyzers.html")

## 2.默认的分词器介绍

ES的默认分词设置是standard，这个在中文分词时就比较尴尬了，会单字拆分，比如我搜索关键词“清华大学”，这时候会按“清”，“华”，“大”，“学”去分词，然后搜出来的都是些“清清的河水”，“中华儿女”，“地大物博”，“学而不思则罔”之类的莫名其妙的结果。

standard tokenizer：以单词边界进行切分
**standard** token filter：什么都不做
**lowercase** token filter：将所有字母转换为小写
**stop** token filer（默认被禁用）：移除停用词，比如a the it等等

## 3.怎么修改当前默认的分词器

-   修改配置文件

例，想要改变成配置好的ik分词器， 在config/elasticsearch.yml文件中添加如下配置即可：

index.analysis.analyzer.default.type:ik

前提当然是你已经安装了ik分词。

-   请求

```powershell
# 修改默认分词方法(这里修改school_index索引的默认分词为：ik_max_word)：

PUT /school_index
{
    "settings" : {
        "index" : {
            "analysis.analyzer.default.type": "ik_max_word"
        }
    }
}



# 启用english停用词token filter（其中es_std即为新的分词器）

PUT /my_index
{
    "settings": {
        "analysis": {
            "analyzer": {
                "es_std": {
                    "type": "standard",
                    "stopwords": "english"
                }  
            }
        }
    }
}


# 定制化自己的分词器
# 自定义1：把&改成and
# 自定义2: 停用词指定 the a
# 自定义3: 去掉html标签
# custom代表自定义

PUT /my_index
{
    "settings": {
        "analysis": {
            "char_filter": {
                "&_to_and":{
                    "type":"mapping",
                    "mappings":["&=>and"]
                }
            },
            "filter": {
                "my_stopwords":{
                    "type":"stop",
                    "stopwords":["the","a"] 
                }
            },
            "analyzer": {
                "my_anaylzer":{
                    "type":"custom",
                    "char_filter":["html_strip","&_to_and"],
                    "tokenizer":"standard",
                    "filter":["lowercase","my_stopwords"]
                }
            }
        }
    }
}
```

在自己的type里要用到自己自定义的分词器下面语法

```powershell
PUT /my_index/_mapping/my_type
{
    "properties": {
        "content": {
            "type": "text",
            "analyzer": "my_analyzer"
        }
    }
}
```

## 4.怎么安装新的分词器

比如：ik分词器，有两种ik\_smart和ik\_max\_word。

```纯文本
    ik_smart会将“清华大学”整个分为一个词，而ik_max_word会将“清华大学”分为“清华大学”，“清华”和“大学”，按需选其中之一就可以了。
```

-   安装IK分词器

下载 [github:elasticsearch-analysis-ik](https://github.com/medcl/elasticsearch-analysis-ik/releases "github:elasticsearch-analysis-ik")
选择的**ik分词器版本一定要与你的版本一致**，否则就无法启动es，比如我用的是6.1.4的es，那么ik分词器也一定要6.1.4
下载到elasticsearch/plugin目录下

```bash
 wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.1.4/elasticsearch-analysis-ik-6.1.4.zip

 unzip elasticsearch-analysis-ik-6.1.4.zip
```

-   安装ansj分词器

[**https://github.com/NLPchina/elasticsearch-analysis-ansj/releases**](https://github.com/NLPchina/elasticsearch-analysis-ansj/releases "https://github.com/NLPchina/elasticsearch-analysis-ansj/releases") (和ik一样注意版本号对应)

解压放置 es的安装目录plugins下，重新服务，加载分词器设置

### 🧡设置ansj为默认分词器

```纯文本
不支持动态设置，indices处于开启状态，需要先关闭，在进行设置，设置完成后在打开。
通过API设置的方式不需要重启elsatisearch。线上的集群最好不要重启，加载索引的时间会很久并且会引发一些错误。
```

```纯文本
# curl -XPOST 'localhost:9200/_all/_close'

# curl -XPUT 'http://localhost:9200/_all/_settings?preserve_existing=true' -d '{
  "index.analysis.analyzer.default.type" : "index_ansj",
  "index.analysis.analyzer.default_search.type" : "query_ansj"
}'

# curl -XPOST 'localhost:9200/_all/_open'

6.x版本后执行put命令：
6.x版本以后修改或写入数据到es，都要使用-H'Content-Type: application/json'。

#curl -XPUT -H 'Content-Type: application/json' 'http://localhost:9200/_all/_settings?preserve_existing=true' -d '{
  "index.analysis.analyzer.default.type" : "index_ansj",
  "index.analysis.analyzer.default_search.type" : "query_ansj"
}'
```

或者  在**elasticsearch.yml**加入如下配置:

```纯文本
#默认分词器,索引
index.analysis.analyzer.default.type: index_ansj

#默认分词器,查询
index.analysis.analyzer.default_search.type: query_ansj
```

### ❤️三种不同的索引分词

-   index\_ansj 是索引分词,尽可能分词处所有结果
-   搜索分词 (search\_ansj=to\_ansj=query\_ansj)
    ```纯文本
    query_ansj 是搜索分词,是索引分词的子集,保证了准确率
    ```
-   用户自定义词典优先的分词方式 (user\_ansj=dic\_ansj)

### 🥰提供API

```powershell
# ansj
GET /_cat/ansj/config

GET /_ansj/flush/config

GET /_ansj/flush/dic

GET /_cat/ansj?text=我爱e生保2020&type=index_ansj&dic=dic&stop=stop&ambiguity=ambiguity&synonyms=synonyms&pretty=true

GET /yuan/_analyze
{
    "analyzer": "index_ansj",
    "text": "中药费用e生保2020可不可以报销"
}
```

### ⭐️如何不重启es，使得字典生效

```powershell
先执行 GET /_ansj/flush/config

后执行 GET /_ansj/flush/dic

# 新建index的时候新的词典就生效了

# 之前建的index新增数据时会按新的词典进行操作，如需把之前的数据也更新，则需要reindex，重新analyse
```

[https://www.elastic.co/cn/blog/dictionary-update-behavior-for-elasticsearch-cjk-language-analyzers](https://www.elastic.co/cn/blog/dictionary-update-behavior-for-elasticsearch-cjk-language-analyzers "https://www.elastic.co/cn/blog/dictionary-update-behavior-for-elasticsearch-cjk-language-analyzers")

## 5.加载自己定义的分词模型文件

比如加载pyltp的分词模型文件

## 6.加自定义词典

## 7.如何获取analysis下所有支持的内容

Level1: analysis

Level2:analyzer + char\_filter + filter + tokenizer

具体的只能通过文档一层层的找

## 8.建库插数据之后再修改分词器，怎么更新库

用新的分词器mapping一个新的index出来

然后用reindex请求把旧index数据导入到新的来
