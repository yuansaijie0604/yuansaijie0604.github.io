# Transformer XL

CMU联合Google Brain推出Transformer-XL：Attentive Language Models beyond a Fixed-Length Context

![](images/理论-transformer-xl.png)

## 理论

复习[transformer](wen-ben-mo-xing/transformer.md)

### vanilla Transformer
![](images/vanilla-transformer.png)






### 引入循环机制
![](images/transformer-xl.png)








### 引入相对位置编码






## 源码

### 相对位置编码怎么实现



### memory部分是如何处理的



### attention分数如何计算




#### 怎么理解_rel_shift函数





## 参考

[论文](https://arxiv.org/abs/1901.02860)
[huggingface](https://huggingface.co/docs/transformers/model_doc/transfo-xl#transformer-xl)
https://blog.csdn.net/magical_bubble/article/details/89060213


